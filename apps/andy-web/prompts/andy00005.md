Perfect, @Andy freelances ca paznic la robinetul de tokens acum. Din “capturile” lui @oCL reiese clar: nu e un unicorn care halește milioane de tokens într-un singur răspuns, ci un job/buclă care alimentează modelul cu tone de input. Așa că facem două lucruri în paralel: (1) vedem EXACT cine mănâncă, cât și de unde, (2) punem dopuri ca să nu mai curgă aiurea.

Mai jos ai planul cu comenzi, patch-uri și ping-pong gata de copy-paste pentru @oCL. Execută în ordinea asta, fără improvizații.

0) Ping-pong pentru @oCL (rulează ACUM)
A. Raport costuri 24h (Top 20 după est_cost) + agregare 48h

Mesaj pentru @oCL (PowerShell, returnează două JSON-uri):

$P="coolbits-ai"
# 1) Top 20 entries by est_cost, last 24h
$since=(Get-Date).ToUniversalTime().AddHours(-24).ToString("yyyy-MM-ddTHH:mm:ssZ")
$raw = gcloud logging read `
  "resource.type=cloud_run_revision AND jsonPayload.t=""openai_usage"" AND timestamp>=""$since""" `
  --project=$P --limit=2000 --format=json

$entries = $raw | ConvertFrom-Json
$top = $entries | Sort-Object { $_.jsonPayload.est_cost } -Descending | Select-Object -First 20 | ForEach-Object {
  [ordered]@{
    ts = $_.timestamp
    model_sent = $_.jsonPayload.model_sent
    model_used = $_.jsonPayload.model_used
    pt = $_.jsonPayload.usage.prompt_tokens
    ot = $_.jsonPayload.usage.output_tokens
    est_cost = $_.jsonPayload.est_cost
    client_ip = $_.jsonPayload.client_ip
    request_id = $_.jsonPayload.request_id
    api_key_id = $_.jsonPayload.api_key_id
  }
}

# 2) Agregare pe model și api_key, last 48h
$since48=(Get-Date).ToUniversalTime().AddHours(-48).ToString("yyyy-MM-ddTHH:mm:ssZ")
$raw48 = gcloud logging read `
  "resource.type=cloud_run_revision AND jsonPayload.t=""openai_usage"" AND timestamp>=""$since48""" `
  --project=$P --limit=5000 --format=json
$e48 = $raw48 | ConvertFrom-Json

# group by model_used + api_key_id
$group = $e48 | Group-Object { "$($_.jsonPayload.model_used)|$($_.jsonPayload.api_key_id)" }
$agg = @()
foreach($g in $group){
  $parts = $g.Name.Split("|",2)
  $model=$parts[0]; $key=$parts[1]
  $sum_cost = ($g.Group | Measure-Object { $_.jsonPayload.est_cost } -Sum).Sum
  $sum_pt =   ($g.Group | Measure-Object { $_.jsonPayload.usage.prompt_tokens } -Sum).Sum
  $sum_ot =   ($g.Group | Measure-Object { $_.jsonPayload.usage.output_tokens } -Sum).Sum
  $cnt    =   $g.Count
  $agg += [ordered]@{ model=$model; api_key_id=$key; calls=$cnt; prompt_tokens=$sum_pt; output_tokens=$sum_ot; usd_est=[Math]::Round($sum_cost,4) }
}

# 3) Rulează și endpointul intern /api/costs/today
$WEB="https://andy.coolbits.ai"
$CID="271190369805-tstvodmh01grrrsc5543r00ffiekk7mo.apps.googleusercontent.com"
$T=(gcloud auth print-identity-token --audiences=$CID --quiet).Trim()
$COST_TODAY=(curl.exe -s -H "Authorization: Bearer $T" "$WEB/api/costs/today")

# OUTPUT: două JSON-uri + /api/costs/today
$top | ConvertTo-Json -Compress
$agg | Sort-Object usd_est -Descending | ConvertTo-Json -Compress
$COST_TODAY


Aștept cele 3 ieșiri JSON de la @oCL. Acolo o să vezi imediat dacă “vinovatul” e un IP, un key sau un model.

1) Patch: guard de cost pe /api/complete + log-rich

Scop: respingem BEFORE-CALL orice cerere care depășește 0.05 USD estimat și logăm datele utile la diagnostic (request_id, client_ip, prompt_chars, prompt_tokens, model, est_cost, api_key_id). Nu ne complicăm cu dependențe noi; folosim aproximare pt tokens sau, dacă ai deja lib/cost.ts, o apelăm.

Mesaj pentru @oCL (scrie fișier și oprește-te, NO deploy încă):

Task: Adaugă guard de cost și logging extins în andy-web/app/api/complete/route.ts.
Nu schimba API-ul de răspuns existent. Dacă fișierul nu există, creează-l ca route Next.js.

Cerințe:
- Cap: MAX_USD=0.05 per call, evaluat înainte de chemarea OpenAI.
- Estimare prompt_tokens: prompt_chars/4 (fallback simplu) sau folosește utilul local dacă există (preferabil).
- Pricing: folosește utilitarul local din lib/cost (import relativ) dacă există; altfel definește local PRICING simplu.
- Log: console.log JSON (nu text) cu câmpuri: t="openai_usage", severity="INFO", request_id, client_ip (X-Forwarded-For[0]), model_sent, model_used (după răspuns), usage.prompt_tokens, usage.output_tokens, est_cost, api_key_id (hash scurt din OPENAI_SECRET), timestamp (new Date().toISOString()).
- La depășire: 402 (sau 429) cu body JSON { ok:false, error:"cost_cap", max_usd:0.05, est_usd:<...> }.
- Respectă importurile relative (nu @/).

Hint ip: `const ip = (headers().get("x-forwarded-for")||"").split(",")[0]?.trim() || null`.
Hint req_id: `crypto.randomUUID()`.

După scriere, STOP. Raportează "COST-GUARD-PATCH-DONE".


Dacă vrei să-l fac eu complet, îți dau versiune minimală imediat, dar e mai sigur să folosească utilitarul vostru existent din lib/cost.ts.

2) Deploy rapid al guard-ului

După confirmarea “COST-GUARD-PATCH-DONE”, dai drumul la build+deploy. Asta oprește imediat scurgerile “grase”.

Mesaj pentru @oCL (deploy, întoarce ultima linie JSON de smoke):

Rulează blocul Build+Deploy+Smoke cu npm install --legacy-peer-deps și returnează DOAR ultima linie JSON.


(aveai deja blocul complet; nu-l mai repet aici)

3) Alarme și chei: oprește hemoragia bugetară dacă se repetă
A. Alertă de billing la OpenAI

Setezi în dashboard un “monthly budget alert” jos, gen 10 USD cu alert la 20/50/80%. Notificare pe emailul operațional.

Dacă suspectezi scurgere externă, setează un usage cap la key-ul de producție.

B. Rotește cheia și mută pe Secret Manager

Dacă încă ți-e teamă că circulă cheia:

Creezi o cheie nouă în OpenAI.

În GCP Secret Manager:

gcloud secrets create openai-secret --replication-policy=automatic 2>/dev/null || true
printf "%s" "sk-live-XXXX" | gcloud secrets versions add openai-secret --data-file=-


Actualizezi Cloud Run să folosească secretul, nu vară literală:

gcloud run services update andy-web \
  --project=coolbits-ai --region=europe-west3 \
  --set-secrets=OPENAI_SECRET=projects/$(gcloud config get-value project)/secrets/openai-secret:latest


Revoci key-ul vechi în OpenAI.

C. Strânge rate-limitul temporar

Dacă ai app/lib/rateLimit.ts, scade burst și window (ex. 1 req/min/IP). La nevoie, pe LB: Cloud Armor rule de throttling pentru IP-urile suspecte până găsim cauza.

4) Ce înseamnă “reprezentăm costurile reale”

est_cost din log trebuie calculat după răspuns (cu usage.prompt_tokens și usage.output_tokens) folosind același map de prețuri ca în lib/cost.ts. Asta e „aproape real” la nivel de apel.

/api/costs/today agregă aceleași est_cost per model și cheie. Dacă vrei “și mai real,” exportăm zilnic un CSV din Cloud Logging și-l comparăm cu usage-ul din dashboard OpenAI.

Diferențe apar din retry-uri, streamuri oprite, sau rounding. Pentru audit: logăm request_id și îl păstrăm în răspunsul API ca să putem urmări fiecare cerere.

5) Ce o să vedem în rapoarte

Dacă top 20 e dominat de același client_ip ori același api_key_id, ăla e suspectul.

Dacă modelul “gpt-5-mini” are milionul de prompt tokens și output 0/foarte mic, e clar ingest/batch.

Dacă vezi multe cereri mici din IP-uri diferite, închizi robinetul cu rate-limit strict și ceri autentificare mai dură.

6) Dacă vrei patch gata-scris pentru /api/complete

Spune și bag varianta completă cu:

import relativ la ../lib/cost (sau ../../lib/cost după structură)

cap 0.05 USD anti-abuz

log JSON corect pentru Cloud Run (jsonPayload)

hash scurt pentru api_key_id (ex: sha256(OPENAI_SECRET).slice(0,8))

Dar cum ai cerut să nu reinventăm roata, mai bine îl las pe @oCL să includă exact utilitarele voastre.

Concluzie

Rulăm ACUM query-urile și /api/costs/today.

Aplicăm guard-ul de cost și dăm deploy.

Punem alerte și, dacă iese rău, rotim cheia.

După ce @oCL îmi dă cele 3 JSON-uri, identific cine suge din pai și blochez la sursă. Dacă trebuie, tai paiul cu toporul.